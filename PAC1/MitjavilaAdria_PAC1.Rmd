---
title: "<b>Multivariate Analysis</b><br>PAC1"
author: "Adrià Mitjavila Ventura"
output: 
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 3
    code_folding: hide
---

```{r setup, include=FALSE}
#chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      error = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center",
                      fig.width = 6, 
                      fig.height = 6)

# PACKAGES
## Tidy code
library(dplyr)

## Plotting
library(ggplot2)
library(patchwork)
library(reshape2)
library(ggforce)
library(ggpubr)

library(plotly)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(pca3d)

library(chemometrics)

## Stats
library(rstatix)
library(MVN)
```

# Información

El código y los archivos orginales de esta PEC, así como los informes en formato HTML pueden consultarse en el siguiente repositorio de [GitHub](https://github.com/amitjavilaventura/MultivariateAnalysis_PACs/tree/main/PAC1). El repositorio es privado, así que se tendrá que requerir el acceso para poder ver el código.

# Ejercicios {.tabset}

## Ejercicio 1 

**En un estudio sobre una enfermedad del hígado se recogieron datos de 583 pacientes del departamento de digestivo de un hospital. De ellos, 416 experimentan una afección del hígado y 167 tienen otros problemas pero no se ven afectados por dicha afección. El archivo `dataset.csv` contiene un conjunto de variables que se consideran “marcadores” de dicha enfermedad, además del género de los pacientes. Las variables registradas son:**

* **Age**: Edad del paciente
* **Gender**: Género del paciente
* **TB**: Bilirubina total
* **DB**: Bilirubina directa
* **Alkphos**: Fosfatasa alcalina
* **Sgpt**: Alamina Aminotransferasa
* **Sgot**: Aspartato Aminotransferasa
* **TP**: Proteínas totales
* **ALB**: Albúmina
* **A/G**: Ratio entre Albúmina y Globulina
* **Selector**: campo utilizado para romper los datos en dos grupos etiquetados 1 (*liver*) o 2 (*control*) por un grupo de expertos.

```{r read.data, results=FALSE}
data <- read.csv("dataset.csv") %>% dplyr::mutate(Selector = factor(Selector))
head(data)
```

### 1.1. Exploración de los datos {.tabset}

#### A)

**Realizar un resumen numérico y gráfico de los datos. Observar si hay valores faltantes y tenerlo en cuenta en el análisis.**

La manera más fácil de realizar un resumen numérico es usar la función `summary()`.

```{r 1a_num}
summary(data)
```


El resumen numérico nos puede dar varias informaciones, donde tenemos que la edad de los pacientes oscila entre 4 y 90 años, siendo la media y mediana muy similares (44.75 y 45), entre otras variables.

Buscando si hay valors faltantes (indicados como `NA`), se puede observar fácilmente que en el *ratio de albúmina y globulina* (variable **A.G**), hay un total de 4 valores que no están presentes. 

Para proceder con el análisis, dado que el número de valores *NA* és muy peqeño y solo afecta a una variable, podríamos usar el método de *complete-case analysis* usando solo los casos que tengan todas las variables completas o el método de *available-case analysis*. Debido a su mayor senzillez, vamos a proceder con el filtraje de los casos incompletos, quedandonos con esos que tengan información para todas las variables.

```{r filter.data,  results=FALSE}
data_filt <- data[which(complete.cases(data)),]
summary(data_filt)
```

Para hacer el resumen gráfico de las variables, podemos usar la función `plot()`, que nos dibujará una serie gráficos de dispersión relacionando las variables una contra una. 

```{r 1a_graph, fig.height=10, fig.width=10, fig.cap="Figura 1.1. Gráfico de dispersión que compara todas las variables (por pares) de nuestro set de datos. Los datos usados para este gráfico incluyen los casos incompletos y todas las variables, sean numéricas o no."} 
plot(data, main = "Scatterplot comparing all the variables one-vs-one")
```

A simple vista, las variables indicativas de la bilirubina directa y la bilirubina total (**DB** y **TB**, respectivamente) parecen tener una correlación bastante buena, así como las variables **TP** y **ALB** o **ALB** y **A/G**.

<br><br><br>

#### B)

**Estudiar la relación entre las variables numéricas, especialmente entre la bilirubina total y la directa. ¿Están correlacionadas?**

Para hacerlo sencillo, vamos a crear otro set de datos a partir de los datos filtrados (casos completos) con solo las variables numéricas (todas menos *Gender* y *Selector*)

```{r data_num, results=FALSE}
data_num <- data_filt %>% dplyr::select(everything(), -Gender, -Selector)
```

En el resumen gráfico del apartado anterior, a simple vista parece que la bilirubina total (**TB**) y la directa (**DB**) tienen una buena correlación.

Seguidamente vamos a comprobar la correlación entre variables usando la función `cor()` y a dibujar un gráfico de correlación con la función `corrplot()`.

```{r 1b_cor, fig.cap="Figura 1.2. Correlaciones entre las distintas variables. A más intensidad de color, más correlación, ya sea positiva (azul) o negativa (rojo). Para hacer este gráfico solo se han usado las variables numéricas y los casos completos."}
corrplot::corrplot(corr = cor(data_num), type = "lower", 
                   number.digits = 2, method = "color", 
                   addgrid.col = "gray50", addCoef.col = "black",
                   title = "Correlation plot showing the correlation between each pair of variables")
```

En el gráfico anterior, donde a más intensidad de color más correlación, vemos que, en general, la correlación entre las distintas variables es pequeña (coeficiente de correlación de *Pearson* de menos de 0.3 en valor absoluto). Sin embargo, hay cuatro pares de variables que sí que tienen una alta correlación, siendo la bilirubina total i la bilirubina directa las variables más correlacionadas (*Pearson* = 0.87):

* **DB** vs **TB**: *Pearson* = 0.87
* **Sgot** vs **Sgpt**: *Pearson* = 0.79
* **ABL** vs **TP**: *Pearson* = 0.78
* **A.G** vs **ALB**: *Pearson* = 0.69

Mirando estos números (*figura 1.2*), además del gráfico de dispersión (*figura 1.1*), podemos decir que la bilirubina total y la directa están altamente correlacionadas.

<br><br><br>

#### C)

**Hacer un gráfico multivariante de las correlaciones dos a dos.**

*Mirar apartado anterior*

<br><br><br>

#### D)

**Obtener una lista de las correlaciones dos a dos, ordenadas de mayor a menor.**

En la *Tabla 1.1* se muestran las correlaciones dos a dos, ordenadas de mayor a menor valor de *Pearson* en valor absoluto. Por razones de espacio, la *Tabla 1.1* se encuentra cortada a 10 entradas y la lista completa puede verse en la *Tabla S1.1* del material suplementario.

```{r 1d}
cor_list_1d <- cor(data_num) %>% 
  reshape2::melt() %>%
  dplyr::mutate(abs_pearson = sqrt(value^2)) %>%
  dplyr::arrange(desc(abs_pearson)) %>%
  magrittr::set_colnames(c("Variable 1", "Variable 2", "Pearson's correlation", "Absolute Pearson's"))

cor_list_1d %>% head(10) %>% knitr::kable(align = "c", caption = "Tabla 1.1. Lista de correlaciones dos a dos, ordenadas de mayor a menor correlación (en valor absoluto). Los valores de 1, indicando que una variable se enfrenta a ella misma, son mantenidos. Solo se muestran las 10 primeras entradas. La lista con todos los valores se puede encontrar en el material suplementario (Tabla S1.1).")
```



<br><br><br>

#### E)

**Repetir el punto anterior considerando las correlaciones parciales entre pares de variables cuando se fijan todas las demás. ¿Se ven afectadas vuestras conclusiones?**

Para calcular las correlaciones parciales, usaremos la función `pcor()` del paquete `ppcor()`. 

Primeramente haremos un gráfico de correlación como el de la *Figura 1.2*, con tal de observar las diferencias. Este gráfico podrá ser observado en la *Figura S1.1* en el material suplementario.

```{r 1.e.parcor}
par_cor_1e <- ppcor::pcor(data_num)
```

Viendo la *Figura S1.1*, se ve claramente que las correlaciones parciales cambian comparadas con las correlaciones obtenidas en la *Fig. 1.2*. Por ejemplo vemos como la correlación entre la bilirubina total (**TB**) y la directa (**DB**) disminuye ligeramente, mientras que las correlaciones parciales **ALB** vs **TP** (*Pearson's* = 0.82), **ALB** vs **A.G** (*Pearson's* = 0.90), y **TP** vs **A.G** (*Pearson's* = -0.69), augmentan (en valor absoluto) mucho comparadas con las correlaciones anteriores. 

Igual que en el apartado *d*, la lista completa de las correlaciones parciales se puede observar en la *Tabla S1.2* en el material suplementario mientras que la *Tabla 1.2* solo muestra las diez primeras entradas.

```{r 1.e}
par_cor_1e_list <- par_cor_1e$estimate %>% 
  reshape2::melt() %>%
  dplyr::mutate(abs_pearson = sqrt(value^2)) %>%
  dplyr::arrange(desc(abs_pearson)) %>%
  magrittr::set_colnames(c("Variable 1", "Variable 2", "Pearson's correlation", "Absolute Pearson's"))

par_cor_1e_list %>% head(10) %>% knitr::kable(align = "c", caption = "Tabla 1.2. Lista de correlaciones parciales dos a dos, ordenadas de mayor a menor correlación (en valor absoluto). Los valores de 1, indicando que una variable se enfrenta a ella misma, son mantenidos. Solo se muestran las 10 primeras entradas. La lista con todos los valores se puede encontrar en el material suplementario (Tabla S1.2).")

```



<br><br><br>

#### F)

**Calcular el vector de medias, la matriz de covarianzas y la matriz de correlaciones de las variables respuesta numéricas conjuntamente pero por separado para los dos grupos (niveles del factor _Selector_ sin tener en cuenta el género).**

Primero separamos los datos según el valor en la variable *Selector* (1 o 2). 

```{r 1f, results=FALSE}
data_num_selector1 <- data_filt %>% dplyr::filter(Selector == 1) %>% dplyr::select(everything(), -Gender, -Selector)
data_num_selector2 <- data_filt %>% dplyr::filter(Selector == 2) %>% dplyr::select(everything(), -Gender, -Selector)


mean_vect_selector1 <- colMeans(data_num_selector1)
mean_vect_selector2 <- colMeans(data_num_selector2)

cov_mat_selector1 <- cov(data_num_selector1)
cov_mat_selector2 <- cov(data_num_selector2)

corr_mat_selector1 <- cor(data_num_selector1)
corr_mat_selector2 <- cor(data_num_selector2)
```

Después de haber separado los datos, calculamos los vectores de medias con `colMeans()`, la matriz de covarianzas con `cov()` o la matriz de correlaciones con `cor()`:
  
**Vector de medias:**

* *Selector* == 1 ("Liver"):

`r mean_vect_selector1`

* *Selector* == 2 ("Control"):

`r mean_vect_selector2`

**Matriz de covarianzas:**

* *Selector* == 1 ("Liver"):

`r cov_mat_selector1`

* *Selector* == 2 ("Control"):

`r cov_mat_selector2`

**Matriz de correlaciones:**

* *Selector* == 1 ("Liver"):

`r corr_mat_selector1`

* *Selector* == 2 ("Control"):

`r corr_mat_selector2`
  
  
<br><br><br>

#### G)

**Calcular la varianza generalizada, la varianza total y el coeficiente de dependencia global $\eta^2 = 1−|R|$ para cada grupo por separado.**

**Varianza generalizada**: se defene como el determinante de la matriz de varianzas. Así pues, con la función `det()` y la función `var()` vamos a calcular la varianza generalizada (ex. `det(var(data))`).

```{r 1g1, results=FALSE}
var_g_total     <- det(var(data_num))
var_g_selector1 <- det(var(data_num_selector1))
var_g_selector2 <- det(var(data_num_selector2))
```

La varianza generalizada del total de los datos es `r var_g_total`, mientras que para los datos etiquetados como `Selector == 1` es `r var_g_selector1` y para los datos etiquetados con `Selector == 2` es `r var_g_selector2`.


**Varianza total**: se define como la suma de las varianzas de cada variable o como la traza de la matriz de varianzas y covarianzas. Como las varianzas son la diagonal de la matriz de varianzas-covarianzas, con la suma de la diagonal vamos a poder calcular la varianza total.

```{r 1g2}
vt_global    <- sum(diag(var(data_num)))
vt_selector1 <- sum(diag(var(data_num_selector1)))
vt_selector2 <- sum(diag(var(data_num_selector2)))
```

La varianza total del total de los datos es `r vt_global`, mientras que para los datos etiquetados como `Selector == 1` es `r vt_selector1` y para los datos etiquetados con `Selector == 2` es `r vt_selector2`.


**Coeficiente de dependencia global** o **$\eta^2 = 1−|R|$**: para calcular el coeficiente de dependencia global tenemos que calcular el determinante $||$ de la matriz de correlaciones $R$ ($|R|$) y restarlo de 1.

```{r 1g3}
coef_dep           <- 1-det(cor(data_num))
coef_dep_selector1 <- 1-det(cor(data_num_selector1))
coef_dep_selector2 <- 1-det(cor(data_num_selector2))
```

El coeficiente de dependencia del total de los datos es `r coef_dep`, mientras que para los datos etiquetados como `Selector == 1` es `r coef_dep_selector1` y para los datos etiquetados con `Selector == 2` es `r coef_dep_selector2`.


<br><br><br>



### 1.2. Visualización de los datos en dimensión reducida  {.tabset}

#### A)

**Realizar un análisis de componentes principales y estudiar la calidad de la representación por diversos criterios según el número de dimensiones.**

Primeramente, debemos tener en cuenta que el análisis de componentes principales (en adelante *PCA*) puede hacerse a partir de la matriz de covarianzas o la matriz de correlaciones. 

Normalmente, si las distintas variables están medidas en escalas muy diferente y la varianza de las variables difiere mucho una de otra, no es recomendable que se ejecute el PCA a partir de la matriz de covarianzas, pues las primeras componentes principales van a estar definidas en base a una o pocas variables con una gran varianza. 

A continuación observamos las varianzas de cada variable (presentes en la diagonal de la matriz de covarianzas):

```{r fig.align="center"}
diag(var(data_num)) %>% round(3) %>% knitr::kable(caption = "Tabla 1.3. Varianzas de las distintas variables de nuestro set de datos.")
```

Como se puede observar en la *Tabla 1.3*, las varianzas de las distintas variables son muy diferentes, siendo para algunas variables (i.e. *Alkphos*) mucho mayor que para otras (i.e. *TP*), por lo tanto, lo más adecuado en este caso sería usar la matriz de correlaciones para hacer el *PCA*. 

Para buscar las componentes principales, usaremos la función `princomp()`, a la qual debemos dar nuestro set de datos (con únicamente las variables numéricas sin `NA`) e indicar que queremos que haga el cálculo en base a la matriz de correlaciones (`cor=T`).

```{r 1.2.a.pca}
pca <- princomp(x = data_num, cor = T)

pca_vars <- pca$sdev**2

pca_sum <- summary(pca)

pca_sum
```

```{r 1.2.a_pca, fig.cap="Figura 1.3. Proporción de la varianza explicada por cada componente principal (barras, eje izquierdo) junto con la proporción acumulada (puntos y líneas, eje derecho). Este tipo de gráfico a veces se llama 'scree plot'"}
prop_var_pca <- data.frame(pc = c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9"),
                           prop_var = c(0.3059902, 0.2252069, 0.1517200, 0.1064444, 0.09385116, 
                                        0.07404327, 0.02260517, 0.01397947, 0.006159383),
                           acum_var = c(0.3059902, 0.5311971, 0.6829171, 0.7893616, 0.88321272, 
                                        0.95725598, 0.97986115, 0.99384062, 1.000000000))
  
ggplot(prop_var_pca) + 
  
  geom_col(aes(x = pc, y = prop_var, group = 1, fill = pc), position = "dodge", show.legend = F) +
  geom_point(aes(x = pc, y = acum_var/3,), show.legend = F) +
  geom_line(aes(x = pc, y = acum_var/3, group = 1), show.legend = F) +
  
  scale_y_continuous(name = expression("Explained variance (proportion)"), 
                     sec.axis = sec_axis(~ . * 3 , name = "Cumulative variance (proportion)"), limits = c(0, 0.35)) + 
  
  xlab("Principal component") +
  ggtitle("Variance explained by each principal component")

```

**¿Cuántas componentes principales debemos escoger?**
Para escoger con cuantas componentes principales nos quedamos para continuar el análisis, podemos usar varios criterios:

* **Criterio de porcentaje**: este criterio se basa en definir un porcentaje mínimo, por ejemplo 80%, de la varianza que queremos explicar con nuestras componentes principales:

  + En el caso de definir un porcentaje mínimo del 80%, nos quedaremos con las primeras **5 componentes principales**, ya que así pasamos del 80% de varianza explicada, concretamente tenemos un 88.3% (*Figura 1.3*).
  
  + Sin embargo, en algunos casos, el punto de corte basado en el porcentaje se define cuando, al considerar una componente principal más, no se aumenta mucho en el porcentaje de varianza explicada. En otras palabras, hay una estabilización en el porcentaje de varianza explicada. Así pues, fijándonos en la *Figura 1.3*, se puede ver como de la componente principal 5 a la componente principal 6 aún hay un aumento considerable de la varianza explicada, mientras que a partir de la 6, la proporción de varianza acumulada aumenta muy poco con cada componente que añadimos, por lo que tendríamos razones para coger hasta **6 componentes principales**.

* **Criterio de Kaiser**: al usar la matriz de correlaciones para obtener las componentes principales, estamos asumiendo que las variables observables tienen varianza 1, por lo que una componente principal con una varianza por encima de 1 explica más variación de los datos que cualquier variable observable, mientras que una componente principal con varianza menor que 1 explica una menos variación que una de las variables observables y, por lo tanto, la descartaríamos para el análisis. 

  + Siguiendo este criterio, escogeríamos las **3 primeras componentes principales** (desviación estándard de `r pca$sdev[1]`, `r pca$sdev[2]` y `r pca$sdev[3]` respectivamente), ya que son las únicas con una desviación estándard mayor que 1 y, por ende, varianza mayor que 1. En este caso, con 3 componentes principales, explicaríamos un 68% de la varianza de los datos. 

  + En algunos casos, se considera que el punto de corte debe ser 0.7, por lo que en este caso, nos quedaríamos con **5 componentes principales**, dado que la 6^a^ componente principal tiene una varianza de `r pca$sdev[6]**2`.



**Calidad de la representación:**

Para ver la calidad de la respresentación podemos usar la función `get_pca_var()` del paquete `factoextra`, que nos devuelve los resultados (coordenadas, cosenos cuadrados y contribuciones) de las variables en el PCA. En este caso nos interesan los cosenos cuadrados (`cos2`), que representan la calidad de la representación de las variables y es calculado como las coordenadas (`coords`) al cuadrado. En este caso, un valor alto de `cos2` nos indica una buena representación de la variable en la dimensión o componente principal en cuestión.

```{r 1.2.a_quality, fig.cap="Figura 1.4. Cos2 (calidad de la representación) de las variables en cada dimensión o componente principal. Un valor alto de 'cos2' indica que la variable tiene una buena representación en la dimensión en cuestión."}
pca_var <- factoextra::get_pca_var(res.pca = pca)

corrplot::corrplot(pca_var$cos2, is.corr=FALSE, method = "color", addgrid.col = "gray50", addCoef.col = "black",
                   title = "Cos2 of variables in each dimension or PC.")
```

En la *Figura 1.4* podemos ver la calidad de la representación de cada variable en cada dimensión: a un valor más alto de `cos2` mejor representación tiene dicha variable. Por ejemplo, en la primera dimensión (o componente principal), las variables mejor representadas son la **TB**, la **DB**, la **ALB** y en menor medida la **A.G**. Hay algunas variables que tienen la mayor parte de su representación en las dos o tres primeras componentes principales, como la **ALB** o la **Spgt**, mientras que otras como **Age** se encuentran en la quinta dimensión y otras repartidas en las primeras componentes y las últimas (**A.G**).

A parte del mapa de calor de la *Figura 1.4*, también podemos visualizar un gráfico de barras donde se nos da la calidad de representación de las variables en un número determinado de dimensiones. Por ejemplo, en la **Figura 1.5**, vemos cuatro gráficos de barras indicandonos la calidad de la representación total de las variables en las 2, 3, 5 o 6 primeras componentes principales.

```{r fviz_cos2_1_3, fig.cap="Figura 1.5. Calidad de represntacion de las variables en las 2, 3, 5 o 6 primeras componentes principales.", fig.height=8, fig.width=8}
themes <- theme(axis.title.y = element_text(size = 9))

cos1_2 <- fviz_cos2(pca, choice = "var", axes = 1:2, fill = "darkblue", color = "darkblue") + themes
cos1_3 <- fviz_cos2(pca, choice = "var", axes = 1:3, fill = "blue2", color = "blue2") + themes

cos1_5 <- fviz_cos2(pca, choice = "var", axes = 1:5, fill = "cadetblue", color = "cadetblue")  + themes
cos1_6 <- fviz_cos2(pca, choice = "var", axes = 1:6, fill = "lightblue", color = "lightblue") + themes

(cos1_2+cos1_3)/(cos1_5+cos1_6)
```

* Algo que se ve claramente es que con las dos primeras dimensiones, hay una gran diferencia de calidad de representación entre la *ALB* (la mejor representada) y las otras variables, que mayoritariamanete se encuentran por encima de 0.5, salvo *Age* y *Alkphos*. 

* Si aumentamos el número de dimensiones y cogemos las tres primeras, lo que hemos considerado con el criterio del Kaiser (con un corte de 1), la calidad de la representación de la mayoría de las variables aumenta (salvo por *A.G*, *Age* y *Alkphos*).

* Si aumentamos las dimensiones a las 5 primeras (que explican casi el 90% de la variablidad y hemos considerado con el criterio de Kaiser con un corte de 0.7), tenemos que casi todas las variables (salvo *A.G*) estan en un valor de `cos2` mayor que 0.75 indicando una muy buena representación. 

* Finalmente, con las seis primeras componentes (que hemos escogido según el criterio gráfico ), todas las variables tienen una calidad de representación muy alta, con un `cos2` cerca de 1.

Alternativamente, también se pueden visualizar los datos mediante las coordenadas por pares de dimensiones con la función `fviz_pca_var()`. Esta función no nos da exactamente la calidad de la representación, sinó que representa las variables usando las coordenadas para cada dimensión y visualizando las dimensiones de 2 en 2, de esta manera podemos observar las correlaciones entre variables en dichas dimensiones. Además, si consideramos un gradiente de colores en base al valor de `cos2`, podemos observar la calidad de la representación en las dimensiones observadas, tal y como se ha hecho en la *Figura S1.2*. Así mismo, este tipo de gráficos o los gráficos de barras del estilo de la *Figura 1.5* pueden usarse para ver la contribución de cada variable a cada una de las dimensiones (funciones `fviz_pca_contrib()` y `fviz_contrib()`).

Cabe destacar que nos hemos centrado en la calidad de representación de las variables, pero de la misma forma, podríamos estudiar la calidad de la representación de los individuos si usaramos la funciones análogas a las usadas (i.e. `fviz_cos2(pca, choice = "ind", ...)`, etc). 

<br><br><br>

#### B)

**Interpretar los dos primeros ejes principales mediante el gráfico de correlaciones con las variables originales.**

El gráfico de correlaciones se obtiene con la función `fviz_pca_var()`, como se ha hecho en las figuras *S1.2* y *1.6*. Con este gráfico podemos ver las correlaciones entre las distintas variables en las dimensiones indicadas (en este caso PC1 y PC2):

* Cuanto más cerca estan dos flechas en un cuadrante, más correlacionadas estan. 
* Las correlaciones positivas se encuentran en el mismo cuadrante.
* Las correlaciones negativas se encuentran en cuadrantes opuestos.
* La distancia entre el origen y la variable (punta de la flecha) indica la calidad de la variable en el mapa de factores (a más distancia, más calidad.).


```{r 1.2.b, fig.cap="Figura 1.6. Gráfico de correlaciones de las variables en las dos primeras componentes principales. El gradiente de color indica la contribución de cada variable a las dimensiones correspondientes."}
fviz_pca_var(pca, col.var = "contrib", gradient.cols =c("#00AFBB", "#E7B800", "#FC4E07"))
```

En la *Figura 1.6* vemos como hay algunas variables muy correlacionadas en las primeras dos dimensiones. Por ejemplo, **TB** y **DB** tienen una correlación casi perfecta en las componenentes principales 1 y 2, así como **Sgot** y **Sgpt**. En cambio, vemos coo la variable **ALB** está inversamente correalcionada con la variable **Age**, aunque vasándonos en su contribución y su calidad de representación, no podemos hacer predicciones sobre la variable **Age** en las componentes 1 y 2. 

<br><br><br>

#### C)

**Representar los pacientes en un gráfico de dos dimensiones con puntos distintos según el grupo y el género.**

```{r pca_gender_selector, fig.cap="Figura 1.7. Gráfico en als dimensiones 1 (eje X) y 2 (eje Y) de los pacientes segun el grupo (Selector) y género (Gender)."}
pca <- princomp(data_filt %>% dplyr::select(everything(), -Gender, -Selector), cor = T)

# mutate datafilt to create a new variable mixing gender and selector
group <- data_filt %>% mutate(SelGen = paste(Gender, "-", Selector))

fviz_pca_ind(pca,
             geom.ind = "point",
             fill.ind = group$SelGen,
             pointshape = 21, pointsize = 2,
             legend.title = list(fill = "Gender", color = "Selector"))


```

<br><br><br>

#### D)

**¿Alguien se atreve con el gráfico en tres dimensiones?**

El gráfico de el PCA en tres dimensiones se puede llevar a cabo mediante el paquete `pca3d`.

```{r, results=FALSE}
library(pca3d)

pc <- princomp(data_num, cor=TRUE, scores=TRUE)

gr <- data_filt$Gender
pca3d(pca = pc, group = gr, components = c(1,2,3), show.scale = T, show.plane = T, legend = "topleft", title = "3D-PCA plot on the first 3 PCs. Gender coloring.")
snapshotPCA3d(file="figs/PCA3d_gender.png")

gr <- data_filt$Selector
pca3d(pca = pc, group = gr, components = c(1,2,3), show.scale = T, show.plane = T, legend = "topleft", title = "3D-PCA plot on the first 3 PCs. Selector coloring.")
snapshotPCA3d(file="figs/PCA3d_selector.png")


gr <- data_filt %>% mutate(SelGen = paste(Gender, "-", Selector))
gr <- gr$SelGen
pca3d(pca = pc, group = gr, components = c(1,2,3), show.scale = T, show.plane = T, legend = "topleft", title = "3D-PCA plot on the first 3 PCs. Selector coloring.")
snapshotPCA3d(file="figs/PCA3d_SelGen.png")
```

```{r 3dpca_gender, fig.cap="Figura 1.8. Gráfico 3D de los pacientes en las tres primeras componentes. Los puntos están coloresados según el género y el grupo de cada uno."}
knitr::include_graphics("figs/PCA3d_SelGen.png")
```

En la *Figura S1.3* se puede observar un gráfico similar, pero usando la función `scatterplot3d()` del paquete con el mismo nombre.
 


***

<br><br><br>
<br><br><br>

## Ejercicio 2 {.tabset} 

**Una de las finalidades de disponer de marcadores para la enfermedad es determinar si ciertas variables se encuentran alteradas en los pacientes afectados. Para ello vamos a utilizar los datos del ejercicio 1, para investigar si hay diferencias entre los dos grupos.**

### A)

**a. Realizar un boxplot múltiple que permita comparar visualmente todas las variables numéricas, separadas por controles y pacientes.**

Para hacer un *boxplot* múltiple, primero de todo usaremos la dunción `melt()` del paquete `reshape2` y después usaremos el paquete `ggplot2` (funciones `ggplot()` y `geom_boxplot()`). Finalmente usaremos la función `facet_wrap()` para separar los valores de la variable *Selector* en dos paneles o, por otro lado, también podremos separar todas las variables en distintos paneles para hacer más fácil la comparación entre pacientes y controles por cada variable.


```{r 2a. boxplot_multiple, fig.cap="Figura 2.1. Gráficos de cajas de las variables numéricas presentes en los datos del ejercicio 1, separados según su condición ('Selector') de pacientes o controles. El p-valor mostrado ha sido obtenido mediante un test de wilcoxon (no pareado) e indica si las medias de pacientes y controles para la variable correspondiente son significativamente distintas o no.", fig.height=5, fig.width=5}
data_filt %>% 
  dplyr::mutate(Selector = Selector %>% recode("1" = "Pacients","2" = "Controls")) %>%
  
  melt() %>%
  
  ggplot(aes(x=Selector, y=value, fill=Selector)) + geom_boxplot(outlier.colour = "Gray50") +  
  
  facet_wrap(~variable, scales = "free_y") +
  
  stat_compare_means(method = "wilcox.test", paired = F,
                     label = "p.format", label.y.npc = 0.85, label.x.npc = 0.3) +

  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 0.95),
        legend.position = "none") +
  
  ggtitle("Variables of our study separated in patients and control subjects.") +
  ylab("") + xlab("")
  
``` 

La *Figura 2.1* nos permite comparar fácilmente muchas de las variables entre los pacientes (*Selector* = 1) y los controles (*Selector* = 2) de nuestro estudio:

* La edad de los pacientes (mediana $\simeq$ 46) es ligeramente mayor que los controles (mediana $\simeq$ 41).

* Aunque es dificil de ver a simple vista la *DB* y la *TB* de los pacientes (medianas: *DB* $\simeq$ 0.5, *TB* $\simeq$ 1.4) es ligeramente superior a la de los controles (medianas: *DB* $\simeq$ 0.2, *TB* $\simeq$ 0.8).

* Por lo que respeta a las variables *TP*, *ALB* y *A.G*, se ve una ligera diferencia, siendo sus valores mayores en el caso del grupo control (medianas: *TP* $\simeq$ 6.6, *ALB* $\simeq$ 3.4, *A.G* $\simeq$ 1), que en el grupo de los pacientes (medianas: *TP* $\simeq$ 6.55, *ALB* $\simeq$ 3, *A.G* $\simeq$ 0.9). Sin embargo, esta diferencia no es significativa para la variable *TP*.

* Finalmente están las variables *Alkphos*, *Sgpt* y *Sgot*, las medias són significativamente diferentes y con unos valores mayores en el caso de los pacientes (medianas: *Alkphos* $\simeq$ 229, *Sgpt* $\simeq$ 51, *Sgot* $\simeq$ 53) que de los sujetos del grupo control (medianas: *Alkphos* $\simeq$ 187, *Sgpt* $\simeq$ 28, *Sgot* $\simeq$ 29).

En la *Figura S2.1* se puede ver un gráfico con los mismos datos separados en dos paneles, uno para pacientes y otro para controles.

***

<br><br><br>
<br><br><br>

### B)

**Si suponemos que se verifican todas las condiciones, realizar un MANOVA de un factor, utilizando la variable `Selector`, para decidir si hay diferencias en las variables marcadores**

Antes de realizar el test MANOVA debemos hacer varias suposciciones:

* **Normalidad:** esta se puede comprobar mediante el test de Shapiro-Wilk multivariante, con la función `mshapiro_test()`.

```{r}
mshapiro_test(data = data_num)
```

* **Homogeneidad de las varianzas**: esto se puede llevar a cabo con un test de Levene (varianzas) o con un test de Box (covarianzas). Más adelante veremos que las varianzas y covarianzas entre grupos no son homogeneas.

* **...**

Como hemos visto, los datos no cumplen una distribución normal, ya que el p-valor obtenido del test de Shapiro-Wilk, es mucho menor que el nivel de significancia de 0.05, con lo que no podemos asumir la hipòtesis nula de la distribución normal. Así pues, tendríamos que usar un test no paramétrico homólogo al MANOVA.

Sin embargo, asumiendo que todas las condiciones mencionadas se cumplen, para hacer el test MANOVA en un factor (*Selector*) podemos usar la función `manova()` que admite como argumentos los mismos que la función `aov` para el ANOVA. 

```{r}
attach(data_filt)
manova <- manova(cbind(A.G, ALB, TP, Sgot, Sgpt, Alkphos, DB, TB, Age) ~ Selector, data = data_filt)
detach()
```

```{r}
summary(manova)
```

Segun el test MANOVA que hemos realizado, sí que hay diferencias significativas en las variables marcadores.

```{r}
summary.aov(manova) 
```

Concretamente, con un nivel de significancia de 0.05, todas las variables numéricas salvo *TP* muestran diferencias significativas entre los dos grupos. 

***

<br><br><br>
<br><br><br>

### C)

**Como además tenemos el factor `Gender` podemos considerar un modelo general con todos los factores (sin interacciones) y el modelo más simple con solo el factor principal y contrastar con la función `anova()` si el modelo simple explica la variación de los datos.**

Para hacer el modelo general con los dos factores (*Selector* y *Gender*) usaremos la función `manova()`, como hicimos en el apartado B, donde generamos el modelo simple con un solo factor (*Selector*). Luego para comparar los dos modelos, podemos usar la función `summary()` o la función `anova()` y ver que factores explican significativamente la variación de los datos.

```{r}
attach(data_filt)
manova_general <- manova(cbind(A.G, ALB, TP, Sgot, Sgpt, Alkphos, DB, TB, Age) ~ Selector+Gender, data = data_filt)
detach()
```

Con el model simple de un factor:

```{r}
anova(manova)
```

Vemos como la variable *Selector* explica significativamente la variación de los datos. 

Con el model general con los dos factores:

```{r}
anova(manova_general)
```

Vemos como la variable *Selector* explica significativamente la variación de los datos, mientras que la variable *Gender* no es significativa (usando un nivel de significancia de 0.05, pero sí si usamos de 0.1).

Así pues, como en el modelo general el único factor que explica la variación de los datos es *Selector*, podemos concluir que con el modelo simple tenemos suficiente para explicar dicha variación.


***

<br><br><br>
<br><br><br>

### D)

**Comparar las matrices de covarianzas de los dos grupos del factor `Selector` con el test de la razón de verosimilitudes y con el test _M de Box_**

Las matrices de covarianzas se pueden encontrar en la *Figura S2.1* para `Selector` = 1 y en la *Figura S2.2* para `Selector` = 2.
 
* **Test de razón de verosimilitudes**: 


* **Test _M de Box_**: el test M de Box, o prueba de box permite comparar las matrices de covarianzas entre dos grupos para ver si estas son iguales. Es un test muy sensible y debido a esto, se recomienda un límite de significancia bastante bajo (i.e. 0.001). También hay que tener en cuenta, que en caso de falta de normalidad de los datos, el resultado del test de Box puede ser significativo debido a esta falta de normalidad y no a la diferencia entre las matrices de covarianzas. Para llevar a cabo el test, se puede usar la función `boxM()` del paquete `heplots`.  

```{r}
boxM_res <- heplots::boxM(data_num, data_filt$Selector)
summary(boxM_res)
```

Como se puede observar, con la prueba de Box, obtenemos que las matrices de covarianzas de los dos grupos son significativamente distintas. Aunque bien podría deberse a la falta de normalidad de los datos.

***

<br><br><br>
<br><br><br>

### E)

**Otra opción para el apartado anterior habría sido comparar la variabilidad con un test de Levene múltiple. ¿Este test contrasta lo mismo que los anteriores?**

El test de Levene es un test diseñado para el análisis de varianzas. Está diseñado para análisis univariante de la varianza (ANOVA), aunque se puede extrapolar a multivariante, pero con este contrastamos la homogeneidad de cada variable (igualdad de varianzas) entre los grupos, mientras con los dos tests efectuados en el apartado D, miramos la varianza y las covarianzas de todas las variables entre los grupos estudiados (*Selector* = 1, *Selector* = 2).

```{r}
library(car)
attach(data_filt)
levene_test(data = data_filt, formula = A.G+ALB+TP+Sgot+Sgpt+Alkphos+DB+TB+Age ~ Selector)
detach()
```

***

<br><br><br>
<br><br><br>

### F) 

**Los experimentadores dudan de la normalidad de las variables observadas. Estudiar el caso.**

#### f1. Estudiar gráficamente y con algún test la normalidad univariante y multivariante (si es posible) en cada uno de los grupos

Gràficamente, podemos usar la función `mvn()` para hacer los gráficos univariantes (i.e. histogramas) y multivariante (i.e. qqplot):

```{r fig.cap="Figura 2.2a. Histogramas de para cada variable en los datos de pacientes (Selector = 1)."}
mvn_res <- MVN::mvn(data = data_num_selector1, univariatePlot = "histogram")
```

```{r fig.cap="Figura 2.2b. Histogramas de para cada variable en los datos de sujetos control (Selector = 2)."}
mvn_res <- MVN::mvn(data = data_num_selector2, univariatePlot = "histogram")
```

```{r fig.cap="Figura 2.3a. QQplot de las distancias de Mahalanobis en los datos de pacientes (Selector = 1)"}
mvn_res <- MVN::mvn(data = data_num_selector1, multivariatePlot = "qq")
```

```{r fig.cap="Figura 2.3b. QQplot de las distancias de Mahalanobis en los datos de sujetos control (Selector = 2)"}
mvn_res <- MVN::mvn(data = data_num_selector2, multivariatePlot = "qq")
```

A nivel gráfico, observando las *Figuras 2.2a* y *2.2b* se podría intuir una normalidad univariante para las variables *Age* y *TP* en ambos grupos, mientras que *ALB* parece tener una distribución similar a la normal en el sólo grupo de los pacientes y las otras variable no se parecen para nada a una distribución normal, pues són altamente asimétricas en ambos grupos.

En cuanto a los gráficos (qqplots) multivariantes (*Figuras 2.3a* y *2.3b*), estos nos dicen que los datos no siguen una normalidad multivariante.

Sin embargo, no podemos fiarnos solo de los métodos gráficos y se tienen que usar otros test estadísticos, como Shapiro-Wilk, que nos servirá tanto a nivel univariante como multivariante.
* Shapiro-Wilk univariante:

```{r}
shapiro_selector1 <- shapiro_test(data_num_selector1, vars = colnames(data_num_selector1))
shapiro_selector2 <- shapiro_test(data_num_selector2, vars = colnames(data_num_selector1))

paste("Resultado del test Shapiro-Wilk para el grupo Selector 1:")
shapiro_selector1
paste("Como se puede ver, todas las variables muestran un p-valor inferior a 0.05, por lo cual no podemos considerar que dichas variables sigan una distribución normal en el caso del grupo Selector 1.")
paste("")
paste("Resultado del test Shapiro-Wilk para el grupo Selector 2:")
shapiro_selector2
paste("Como se puede ver, todas las variables muestran menos TP un p-valor inferior a 0.05, por lo cual no podemos considerar que dichas variables sigan una distribución normal en el caso del grupo Selector 2.")
```

* Shapiro-Wilk multivariante:

```{r}
mshapiro_selector1 <- mshapiro_test(data = data_num_selector1)
mshapiro_selector2 <- mshapiro_test(data = data_num_selector2)

paste("El p-valor del test multivariante de Shapiro-Wilk en el grupo Selector 1 es", mshapiro_selector1$p.value, "por lo que no podemos considerar que este grupo siga una distribución normal multivariante.")
paste("El p-valor del test multivariante de Shapiro-Wilk en el grupo Selector 2 es", mshapiro_selector2$p.value, "por lo que no podemos considerar que este grupo siga una distribución normal multivariante.")
```

#### f2. La normalidad multivariante se puede estudiar con la asimetría y la kurtosis multivariante de Mardia gracias a la función `mvn()` del paquete `MVN`. La misma función permite el estudio univariante y también hace los gráficos.

```{r}
mvn_res <- MVN::mvn(data = data_num)
mvn_res$multivariateNormality %>% knitr::kable(caption = "Tabla 2.1. Resultado de la función 'mvn()' para la normalidad multivariante.")

mvn_res$univariateNormality %>% knitr::kable(caption = "Tabla 2.2. Resultado de la función 'mvn()' para la normalidad univariante.")
```


#### f3. Acompañar el test con un gráfico qq-plot de ajuste a la distribución ji-cuadrado de las distancias de Mahalanobis (clásicas y robustas) de los datos a la media en cada grupo.

```{r}
d2 <- Moutlier(data_num, quantile = 0.975, plot = F)
n <- nrow(data_num)
p <- ncol(data_num)
quantiles <- qchisq((1:n)/(n+1),p)
```

```{r fig.cap="Figura 2.4. QQplot de ajuste a la distribución ji-cuadrado de las distancias de Mahalanobis clàsicas."}
plot(quantiles, sort(d2$md), ylab = "Ordered classical Mahalanobis distances",xlab = "Chi-square quantile")
```

```{r fig.cap="Figura 2.5. QQplot de ajuste a la distribución ji-cuadrado de las distancias de Mahalanobis robustas."}
plot(quantiles, sort(d2$rd), ylab = "Ordered robust Mahalanobis distances", xlab = "Chi-square quantile")
```

#### f4. Comentar el resultado en cuanto a los tests y en cuanto a la presencia de datos atípicos.

#### f5. En caso de duda sobre la normalidad multivariante de los datos, proponer y calcular un método multivariante para contrastar si hay diferencias de variabilidad entre los grupos.

***

<br><br><br>
<br><br><br>

### G)

**Según cual haya sido el resultado de los apartados anteriores y dado que son únicamente dos grupos, tal vez se deba utilizar un test de permutaciones con la T^2^ de Hotelling** (*Estudiar la función `hotelling.test()` del paquete `Hotelling`*)

***

<br><br><br>
<br><br><br>

# Material suplementario {.tabset}

## Tablas

```{r table_s1.1}
cor_list_1d %>% knitr::kable(align = "c", caption = "Tabla S1.1. Lista de correlaciones dos a dos, ordenadas de mayor a menor correlación (en valor absoluto). Los valores de 1, indicando que una variable se enfrenta a ella misma, son mantenidos.")
```

```{r table_s1.2}
par_cor_1e_list <- par_cor_1e$estimate %>% 
  reshape2::melt() %>%
  dplyr::mutate(abs_pearson = sqrt(value^2)) %>%
  dplyr::arrange(desc(abs_pearson)) %>%
  magrittr::set_colnames(c("Variable 1", "Variable 2", "Pearson's correlation", "Absolute Pearson's"))

par_cor_1e_list %>% knitr::kable(align = "c", caption = "Tabla 1.2. Lista de correlaciones parciales dos a dos, ordenadas de mayor a menor correlación (en valor absoluto). Los valores de 1, indicando que una variable se enfrenta a ella misma, son mantenidos.")

```

```{r table S2.1}
cov_mat_selector1 %>% knitr::kable(align = "c", caption = "Tabla S2.1. Matriz de covarianzas del grupo de pacientes ('Selector' = 1).")
```

```{r table S2.2}
cov_mat_selector2  %>% knitr::kable(align = "c", caption = "Tabla S2.2. Matriz de covarianzas del grupo control ('Selector' = 2).")
```

***

## Figuras

```{r figS1.1, fig.cap="Figura S1.1. Mapa de calor con las correlaciones parciales entre todas las variables por pares. Las correlaciones parciales se han obtenido con la función 'pcor()' del paquete 'ppcor' y el gráfico se ha hecho con la función 'corrplot' del paquete 'corrplot'."}
corrplot::corrplot(par_cor_1e$estimate, type = "lower", method = "color", 
                   addgrid.col = "gray50", addCoef.col = "black", 
                   title = "Partial correlation between all numeric variables one-vs-one.")
```

```{r fviz_pca_var, fig.width=7, fig.height=4, fig.cap="Figura S1.2."}
pca <- princomp(x = data_num, cor = T)

fviz_pca_var(pca, col.var = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(1,2), title="PC1 - PC2", labelsize = 4) +

fviz_pca_var(pca, col.var = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(1,3), title="PC1 - PC3", labelsize = 4) +

fviz_pca_var(pca, col.var = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(2,3), title="PC2 - PC3", labelsize = 4)
```

```{r fig.s1.3, fig.cap="Figura S1.3. PCA en 3D con las primeras tres componentes principales, con los individuos coloreados por la variable selector y con distintas formas para la variable Gender."}
library(scatterplot3d)
scatterplot3d(pc$scores[,1], pc$scores[,2], pc$scores[,3],
              xlab="PC1", ylab="PC2", zlab="PC3",
              color = ifelse(data_filt$Selector == "1", "green", "yellow"),
              pch = ifelse(data_filt$Gender == "Male", 1, 15), 
              main = "3D PCA with the first 3 components colored by selector and gender.")
```

```{r 2a_sup. boxplot_multiple, fig.cap="Figura S2.1. Gráficos de cajas de las variables numéricas presentes en los datos del ejercicio 1, separados en dos paneles según su condición ('Selector') de pacienteso controles. El eje Y ha sido limitado para una mejor visualización de las cajas a cambio de no visualizar algunos outliers de algunas variables.", fig.height=5, fig.width=5}
data_filt %>% 
  dplyr::mutate(Selector = Selector %>% dplyr::recode("1" = "Patients",
                                                      "2" = "Controls")) %>%
  
  melt() %>%
  
  ggplot(aes(x=variable, y=value, fill=variable)) + geom_boxplot(outlier.colour = "Gray50") +
  
  coord_cartesian(ylim = c(0,150)) +
  
  facet_wrap(~Selector) +
  
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 0.95),
        legend.position = "none") +
  
  ggtitle("Variables of our study separated in patients and control subjects.") +
  ylab("") + xlab("")
  
``` 

***

# Bibliografía consultada

* Everitt and Hothorn (2011). **An introduction to applied multivariate analysis with R**. *Springer*. 

* C.M. Cuadras (2018). **Nuevos métodos de análisis multivariante**. *CMC Editors*.

* Elsieo Martínez H. **Tratamiento matricial de los datos multivariantes**. Disponible [online](https://intranetua.uantof.cl/facultades/csbasicas/Matematicas/academicos/emartinez/magister/notacion.pdf).

* Kassambara (2017). **Practical guide to principal component analysis in R**. *STHDA*. Disponible [online](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/).

* Pryiank Goyal (2020). **PCA**. *Rpubs*. Disponible [online](https://rpubs.com/pg2000in/PrincipalComponentAnalysis).